{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   publish_date                                      headline_text\n",
      "0      20030219  aba decides against community broadcasting lic...\n",
      "1      20030219     act fire witnesses must be aware of defamation\n",
      "2      20030219     a g calls for infrastructure protection summit\n",
      "3      20030219           air nz staff in aust strike for pay rise\n",
      "4      20030219      air nz strike to affect australian travellers\n",
      "Unique Topics:  ['aba decides against community broadcasting licence'\n",
      " 'act fire witnesses must be aware of defamation'\n",
      " 'a g calls for infrastructure protection summit' ...\n",
      " 'wa delays adopting new close contact definition'\n",
      " 'western ringtail possums found badly dehydrated in heatwave'\n",
      " 'what makes you a close covid contact here are the new rules']\n",
      "Topic Distribution: \n",
      " headline_text\n",
      "national rural news                                            983\n",
      "abc sport                                                      718\n",
      "abc weather                                                    714\n",
      "abc business news and market analysis                          585\n",
      "abc entertainment                                              551\n",
      "                                                              ... \n",
      "rio drug gang used alligators to terrify slum                    1\n",
      "research to identify women at risk of premature                  1\n",
      "religious order defends sex abuse handling                       1\n",
      "reigning champion federer advances to us semis                   1\n",
      "what makes you a close covid contact here are the new rules      1\n",
      "Name: count, Length: 1213004, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('abcnews-date-text.csv')\n",
    "\n",
    "# Explore the first few rows to understand the data\n",
    "print(df.head())\n",
    "\n",
    "# Check the unique topics/categories\n",
    "unique_topics = df['headline_text'].unique()\n",
    "print(\"Unique Topics: \", unique_topics)\n",
    "\n",
    "# Check the distribution of articles across topics\n",
    "topic_distribution = df['headline_text'].value_counts()\n",
    "print(\"Topic Distribution: \\n\", topic_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW-Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:\n",
      " ['aba' 'act' 'against' 'ambitious' 'aware' 'be' 'broadcasting' 'community'\n",
      " 'decides' 'defamation' 'fire' 'jump' 'licence' 'must' 'of' 'olsson'\n",
      " 'triple' 'wins' 'witnesses']\n",
      "\n",
      "BOW Matrix:\n",
      "\n",
      "[[1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text=['aba decides against community broadcasting licence','act fire witnesses must be aware of defamation',\n",
    "      'ambitious olsson wins triple jump']\n",
    "#Initialize the vector\n",
    "vectorizer=CountVectorizer()\n",
    "#transform\n",
    "x=vectorizer.fit_transform(text)\n",
    "#get feature names(words)\n",
    "feature_names=vectorizer.get_feature_names_out()\n",
    "#Display BOW-Matrix\n",
    "print('Feature names:\\n',feature_names)\n",
    "print()\n",
    "print('BOW Matrix:\\n')\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names(TF-IDF):\n",
      " ['aba' 'act' 'against' 'ambitious' 'aware' 'be' 'broadcasting' 'community'\n",
      " 'decides' 'defamation' 'fire' 'jump' 'licence' 'must' 'of' 'olsson'\n",
      " 'triple' 'wins' 'witnesses']\n",
      "\n",
      "TF-IDF Matrix:\n",
      " [[0.40824829 0.         0.40824829 0.         0.         0.\n",
      "  0.40824829 0.40824829 0.40824829 0.         0.         0.\n",
      "  0.40824829 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.35355339 0.         0.         0.35355339 0.35355339\n",
      "  0.         0.         0.         0.35355339 0.35355339 0.\n",
      "  0.         0.35355339 0.35355339 0.         0.         0.\n",
      "  0.35355339]\n",
      " [0.         0.         0.         0.4472136  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.4472136\n",
      "  0.         0.         0.         0.4472136  0.4472136  0.4472136\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text=['aba decides against community broadcasting licence','act fire witnesses must be aware of defamation',\n",
    "      'ambitious olsson wins triple jump']\n",
    "\n",
    "tfidf=TfidfVectorizer()\n",
    "x_tfidf=tfidf.fit_transform(text)\n",
    "\n",
    "features_names_tfidf=tfidf.get_feature_names_out()\n",
    "\n",
    "print('Feature Names(TF-IDF):\\n',features_names_tfidf)\n",
    "print()\n",
    "print('TF-IDF Matrix:\\n',x_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a statement:\n",
      "\tcommonwealth bank cuts fixed home loan rates. community urged to help homeless youth commonwealth bank cuts fixed home loan rates.\n",
      "commonwealth bank cuts fixed home loan rates.\n",
      "45\n",
      "community urged to help homeless youth commonwealth bank cuts fixed home loan rates.\n",
      "84\n",
      "\n",
      "Feature Names(TF-IDF):\n",
      " ['bank' 'commonwealth' 'community' 'cuts' 'fixed' 'help' 'home' 'homeless'\n",
      " 'loan' 'rates' 'to' 'urged' 'youth']\n",
      "\n",
      "TF-IDF Matrix:\n",
      " [[0.37796447 0.37796447 0.         0.37796447 0.37796447 0.\n",
      "  0.37796447 0.         0.37796447 0.37796447 0.         0.\n",
      "  0.        ]\n",
      " [0.23031454 0.23031454 0.32369906 0.23031454 0.23031454 0.32369906\n",
      "  0.23031454 0.32369906 0.23031454 0.23031454 0.32369906 0.32369906\n",
      "  0.32369906]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text=input('Enter a statement:\\n\\t')\n",
    "sent_token=sent_tokenize(text)\n",
    "#print('Sentences:\\n',sent_token)\n",
    "for sent in sent_token:\n",
    "    print(sent)\n",
    "    print(len(sent))\n",
    "\n",
    "print()\n",
    "tfidf=TfidfVectorizer()\n",
    "x_tfidf=tfidf.fit_transform(sent_token)\n",
    "\n",
    "features_names_tfidf=tfidf.get_feature_names_out()\n",
    "\n",
    "print('Feature Names(TF-IDF):\\n',features_names_tfidf)\n",
    "print()\n",
    "print('TF-IDF Matrix:\\n',x_tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:aba decides against community broadcasting licence\n",
      "\n",
      "Generated 2-grams:\n",
      "('aba', 'decides')\n",
      "('decides', 'against')\n",
      "('against', 'community')\n",
      "('community', 'broadcasting')\n",
      "('broadcasting', 'licence')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "text='aba decides against community broadcasting licence'\n",
    "tokens=nltk.word_tokenize(text)\n",
    "#Generate bigrams\n",
    "n=2\n",
    "bigrams=list(ngrams(tokens,n))\n",
    "print(f\"Original text:{text}\")\n",
    "print()\n",
    "print(f\"Generated {n}-grams:\")\n",
    "for gram in bigrams:\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:aba decides against community broadcasting licence\n",
      "\n",
      "Generated 3-grams:\n",
      "('aba', 'decides', 'against')\n",
      "('decides', 'against', 'community')\n",
      "('against', 'community', 'broadcasting')\n",
      "('community', 'broadcasting', 'licence')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "text='aba decides against community broadcasting licence'\n",
    "tokens=nltk.word_tokenize(text)\n",
    "#Generate n-grams\n",
    "n=3\n",
    "ngrams=list(ngrams(tokens,n))\n",
    "print(f\"Original text:{text}\")\n",
    "print()\n",
    "print(f\"Generated {n}-grams:\")\n",
    "for gram in ngrams:\n",
    " print(gram)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:aba decides against community broadcasting licence\n",
      "\n",
      "Generated 4-grams:\n",
      "('aba', 'decides', 'against', 'community')\n",
      "('decides', 'against', 'community', 'broadcasting')\n",
      "('against', 'community', 'broadcasting', 'licence')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "text='aba decides against community broadcasting licence'\n",
    "tokens=nltk.word_tokenize(text)\n",
    "#Generate n-grams\n",
    "n=4\n",
    "ngrams=list(ngrams(tokens,n))\n",
    "print(f\"Original text:{text}\")\n",
    "print()\n",
    "print(f\"Generated {n}-grams:\")\n",
    "for gram in ngrams:\n",
    " print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:aba decides against community broadcasting licence\n",
      "\n",
      "Generated 2-grams:\n",
      "['aba', 'decides']\n",
      "['decides', 'against']\n",
      "['against', 'community']\n",
      "['community', 'broadcasting']\n",
      "['broadcasting', 'licence']\n"
     ]
    }
   ],
   "source": [
    "#generate n-grams with list comprehension\n",
    "n=2\n",
    "text='aba decides against community broadcasting licence'\n",
    "tokens=nltk.word_tokenize(text)\n",
    "ngrams=[tokens[i:i+n] for i in range(len(tokens)-n+1)]\n",
    "print(f\"Original text:{text}\")\n",
    "print()\n",
    "print(f\"Generated {n}-grams:\")\n",
    "for gram in ngrams:\n",
    " print(gram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Onehot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "text=['aba decides against community broadcasting licence','act fire witnesses must be aware of defamation',\n",
    "      'ambitious olsson wins triple jump']\n",
    "#step 1- tokens\n",
    "tokens=[word for sent in text for word in sent.lower().split()]\n",
    "#step 2- vocabulary\n",
    "vocabulary=list(set(tokens)) # unique words in the text\n",
    "#initialize encoder\n",
    "encoder=OneHotEncoder(categories=[vocabulary],sparse=False)\n",
    "#Perform the one-hot encoding\n",
    "one_hot_encoded=[]\n",
    "for sent in text:\n",
    "    sent_encoded=[]\n",
    "    for word in sent.lower().split():\n",
    "        word_index=vocabulary.index(word)\n",
    "        word_vector=np.zeros(len(vocabulary))\n",
    "        word_vector[word_index]=1\n",
    "        sent_encoded.append(word_vector)\n",
    "        one_hot_encoded.append(sent_encoded)\n",
    "\n",
    "for sent in one_hot_encoded:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in c:\\users\\nirmala\\appdata\\roaming\\python\\python38\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\program files\\anaconda3\\lib\\site-packages (from gensim) (1.19.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\nirmala\\appdata\\roaming\\python\\python38\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\nirmala\\appdata\\roaming\\python\\python38\\site-packages (from gensim) (6.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp310-cp310-win_amd64.whl (24.0 MB)\n",
      "     ---------------------------------------- 24.0/24.0 MB 5.4 MB/s eta 0:00:00\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\nirmala\\anaconda3\\lib\\site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\nirmala\\anaconda3\\lib\\site-packages (from gensim) (1.26.1)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-6.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'youth':[-0.04309844  0.01832869  0.02594942  0.02870969  0.03733459 -0.03083838\n",
      "  0.00552807  0.03023641 -0.01420025 -0.03086761 -0.00205112 -0.04184474\n",
      " -0.02800006  0.03552269  0.0167627   0.03612835  0.03400124  0.03765371\n",
      " -0.01894577 -0.00280903]\n",
      "====================================================================================================\n",
      "Vector for 'risk':[-0.00788826  0.00160686 -0.02070315 -0.03841344 -0.00754004  0.01234897\n",
      " -0.00444013  0.02766831 -0.01371489  0.01130033  0.02727897  0.04172977\n",
      " -0.0072687  -0.04604071  0.02185276  0.00285892  0.03720954 -0.00406641\n",
      " -0.01319207 -0.04376505]\n",
      "====================================================================================================\n",
      "Similarity b/w 'word' and 'successive':0.029164906591176987\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "#sample text\n",
    "text=['freedom records net profit for third successive',\n",
    " 'funds allocated for domestic violence victims',\n",
    " 'funds allocated for youth at risk']\n",
    "tokenized_text=[word_tokenize(sentence.lower()) for sentence in text]\n",
    "#Train Word2vec model\n",
    "model=Word2Vec(sentences=tokenized_text,vector_size=20,window=5,min_count=1,workers=4)\n",
    "#Find word vectors\n",
    "vector_youth=model.wv['youth']\n",
    "vector_risk=model.wv['risk']\n",
    "#similarity b/w words\n",
    "similarity=model.wv.similarity('funds','successive')\n",
    "print(f\"Vector for 'youth':{vector_youth}\")\n",
    "print('='*100)\n",
    "print(f\"Vector for 'risk':{vector_risk}\")\n",
    "print('='*100)\n",
    "print(f\"Similarity b/w 'word' and 'successive':{similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['freedom', 'records', 'net', 'profit', 'for', 'third', 'successive'], tags=['0']), TaggedDocument(words=['funds', 'allocated', 'for', 'domestic', 'violence', 'victims'], tags=['1']), TaggedDocument(words=['funds', 'allocated', 'for', 'youth', 'at', 'risk'], tags=['2'])]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "#sample text\n",
    "documents=['freedom records net profit for third successive',\n",
    " 'funds allocated for domestic violence victims',\n",
    " 'funds allocated for youth at risk']\n",
    "#Tokenize & tag documents\n",
    "tagged_data=[TaggedDocument(words=word_tokenize(doc.lower()),\n",
    " tags=[str(i)]) for i,doc in enumerate(documents)]\n",
    "print(tagged_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Doc2vec model\n",
    "model=Doc2Vec(vector_size=100,window=2,min_count=1,workers=5,epochs=20)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data,total_examples=model.corpus_count,epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_doc_1=model.infer_vector(word_tokenize(\"freedom records net profit for third successive\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.2350098e-04, -7.9861179e-04, -3.7356646e-04,  4.6900677e-04,\n",
       "        2.1767593e-04,  6.1079836e-04, -3.0939509e-03,  2.4871100e-03,\n",
       "       -6.0130033e-04,  3.0301444e-03, -3.4139729e-03,  1.1866610e-03,\n",
       "        1.5889225e-03, -2.2909276e-03, -5.1902269e-05,  6.6768070e-04,\n",
       "        4.0777368e-03,  2.4608860e-03,  3.4108199e-03,  4.0113581e-03,\n",
       "       -2.6875364e-03,  2.6701731e-03, -3.1437143e-03,  4.7604772e-03,\n",
       "        3.1663741e-03, -4.1005113e-03,  7.5753947e-04,  8.3931972e-04,\n",
       "       -3.2214404e-04,  4.0893694e-03,  3.7424481e-03, -1.8147351e-03,\n",
       "        3.5270357e-03,  8.6473796e-04, -2.7394702e-03, -1.4964852e-03,\n",
       "        4.5600985e-03,  3.7182102e-03,  4.4762864e-04, -3.8822377e-03,\n",
       "        5.4201472e-04, -1.3621954e-03,  2.6261196e-03, -8.1568694e-04,\n",
       "        3.8899891e-03, -3.1245635e-03,  9.5498259e-04,  3.8671680e-03,\n",
       "       -4.5823581e-03, -4.8908186e-03, -4.6046795e-03,  3.7761454e-03,\n",
       "       -1.7385056e-03,  9.6942611e-05,  1.9761352e-03,  2.9432636e-03,\n",
       "        4.8684194e-03, -4.0206783e-03, -1.5767500e-03, -2.6996050e-03,\n",
       "        2.6712797e-03,  3.0403386e-03,  3.7610577e-03,  9.8675198e-04,\n",
       "       -2.2277629e-03,  4.3102825e-04,  8.9978590e-04,  6.5855141e-04,\n",
       "       -2.9927648e-03,  4.2162035e-03,  2.3102949e-03,  4.1790423e-03,\n",
       "        2.9314670e-03, -4.2198952e-03, -2.9086620e-03, -2.4003619e-03,\n",
       "       -4.8068748e-03, -4.2318823e-03, -3.0046406e-03, -3.7307539e-03,\n",
       "       -3.8506449e-03,  2.7810419e-03, -4.8508649e-03,  3.6322020e-03,\n",
       "        3.9126649e-03,  4.7973972e-03,  5.8584090e-04,  2.9047916e-03,\n",
       "        4.1209632e-03, -4.2525129e-03, -4.8238183e-03,  4.2611017e-04,\n",
       "        2.3068539e-03, -2.0341394e-03,  5.8628019e-04, -3.9719185e-03,\n",
       "        3.6717544e-03,  2.4867211e-03, -3.4950189e-03,  3.3577709e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_doc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector for 'freedom records net profit for third successive':[-3.2350098e-04 -7.9861179e-04 -3.7356646e-04  4.6900677e-04\n",
      "  2.1767593e-04  6.1079836e-04 -3.0939509e-03  2.4871100e-03\n",
      " -6.0130033e-04  3.0301444e-03 -3.4139729e-03  1.1866610e-03\n",
      "  1.5889225e-03 -2.2909276e-03 -5.1902269e-05  6.6768070e-04\n",
      "  4.0777368e-03  2.4608860e-03  3.4108199e-03  4.0113581e-03\n",
      " -2.6875364e-03  2.6701731e-03 -3.1437143e-03  4.7604772e-03\n",
      "  3.1663741e-03 -4.1005113e-03  7.5753947e-04  8.3931972e-04\n",
      " -3.2214404e-04  4.0893694e-03  3.7424481e-03 -1.8147351e-03\n",
      "  3.5270357e-03  8.6473796e-04 -2.7394702e-03 -1.4964852e-03\n",
      "  4.5600985e-03  3.7182102e-03  4.4762864e-04 -3.8822377e-03\n",
      "  5.4201472e-04 -1.3621954e-03  2.6261196e-03 -8.1568694e-04\n",
      "  3.8899891e-03 -3.1245635e-03  9.5498259e-04  3.8671680e-03\n",
      " -4.5823581e-03 -4.8908186e-03 -4.6046795e-03  3.7761454e-03\n",
      " -1.7385056e-03  9.6942611e-05  1.9761352e-03  2.9432636e-03\n",
      "  4.8684194e-03 -4.0206783e-03 -1.5767500e-03 -2.6996050e-03\n",
      "  2.6712797e-03  3.0403386e-03  3.7610577e-03  9.8675198e-04\n",
      " -2.2277629e-03  4.3102825e-04  8.9978590e-04  6.5855141e-04\n",
      " -2.9927648e-03  4.2162035e-03  2.3102949e-03  4.1790423e-03\n",
      "  2.9314670e-03 -4.2198952e-03 -2.9086620e-03 -2.4003619e-03\n",
      " -4.8068748e-03 -4.2318823e-03 -3.0046406e-03 -3.7307539e-03\n",
      " -3.8506449e-03  2.7810419e-03 -4.8508649e-03  3.6322020e-03\n",
      "  3.9126649e-03  4.7973972e-03  5.8584090e-04  2.9047916e-03\n",
      "  4.1209632e-03 -4.2525129e-03 -4.8238183e-03  4.2611017e-04\n",
      "  2.3068539e-03 -2.0341394e-03  5.8628019e-04 -3.9719185e-03\n",
      "  3.6717544e-03  2.4867211e-03 -3.4950189e-03  3.3577709e-03]\n",
      "\n",
      "Most similar document:[('1', 0.17154619097709656), ('0', 0.11758178472518921), ('2', 0.0833878144621849)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nirmala\\AppData\\Local\\Temp\\ipykernel_4128\\812068268.py:2: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  similar_doc=model.docvecs.most_similar(positive=[vector_doc_1])\n"
     ]
    }
   ],
   "source": [
    "#find the most similar document\n",
    "similar_doc=model.docvecs.most_similar(positive=[vector_doc_1])\n",
    "print(f\"vector for 'freedom records net profit for third successive':{vector_doc_1}\")\n",
    "print()\n",
    "print(f\"Most similar document:{similar_doc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
